{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2ffe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from numpy import array\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn import linear_model, tree, ensemble\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, precision_score, recall_score, f1_score\n",
    "ROC_scorer = make_scorer(roc_auc_score)\n",
    "F1_scorer = make_scorer(f1_score)\n",
    "Precision_scorer = make_scorer(precision_score)\n",
    "Recall_scorer = make_scorer(recall_score)\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "667ffe8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62667034-063a-4258-b62c-62b3032e05ca",
   "metadata": {},
   "source": [
    "# Define functions for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e5f39a97-88f2-44ea-842d-17224beec29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "1bd19df9-40a3-4998-add8-ecc6e0f4a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "def set_seed(seed_value=101):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_dataloader, val_dataloader, y_val, epochs):\n",
    "    \n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "    \n",
    "    \"\"\"Train the LSTM model.\"\"\"   \n",
    "    # Tracking best validation accuracy\n",
    "    best_accuracy = 0\n",
    "    best_AUC = 0\n",
    "    \n",
    "    train_losses = np.zeros(epochs)\n",
    "    val_losses = np.zeros(epochs)\n",
    "\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    print(f\"{'Epoch':^7} | {'Train Loss':^12} | {'Train Acc':^9} | {'Val Loss':^10} | {'Val Acc':^9} | {'Val F1':^9} |{'Val AUC':^9} |{'Elapsed':^9}\")\n",
    "    print(\"-\"*60)\n",
    "\n",
    "    for epoch_i in tqdm(range(epochs)):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "\n",
    "        # Tracking time and loss\n",
    "        t0_epoch = time.time()\n",
    "        total_loss = 0\n",
    "        epoc_acc = 0\n",
    "        \n",
    "        train_loss = []\n",
    "        #best_p = []\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_input_tbl, b_labels = batch\n",
    "            b_labels = b_labels.view(-1, 1).float().to(device)\n",
    "            b_input_ids, b_input_tbl, b_labels = b_input_ids.to(device), b_input_tbl.to(device), b_labels.to(device)\n",
    "            \n",
    "            model.zero_grad()\n",
    "\n",
    "            # Perform a forward pass. This will return logits.\n",
    "            logits = model(b_input_ids, b_input_tbl)\n",
    "            logits =logits.to(device)\n",
    "\n",
    "            # Compute loss and accumulate the loss values\n",
    "            loss = loss_fn(logits, b_labels)\n",
    "            loss = loss.to(device)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute acc \n",
    "            acc = binary_acc(logits, b_labels)\n",
    "            epoc_acc += acc.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        avg_train_acc = epoc_acc / len(train_dataloader)\n",
    "        \n",
    "        \n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_losses[epoch_i] = train_loss\n",
    "\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if val_dataloader is not None:\n",
    "            # After the completion of each training epoch, measure the model's\n",
    "            # performance on our validation set.\n",
    "            val_loss, val_accuracy, val_f1, val_AUC, prob_list, predict_labels = evaluate(model, val_dataloader, y_val)\n",
    "            \n",
    "            val_losses[epoch_i] = val_loss\n",
    "\n",
    "            # Track the best accuracy\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "                #best_label = predict_labels\n",
    "                \n",
    "            if val_AUC > best_AUC:\n",
    "                best_AUC = val_AUC\n",
    "                #best_p = prob_list\n",
    "                \n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            print(f\"{epoch_i + 1:^7} | {avg_train_loss:^12.6f} | {avg_train_acc:^9.2f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} |{val_f1:^9.4f} | {val_AUC:^9.4f} | {time_elapsed:^9.2f}\")\n",
    "            \n",
    "    print(\"\\n\")\n",
    "    print(f\"Training complete! Best accuracy: {best_accuracy:.2f}%.\")\n",
    "    print(f\"Training complete! Best AUC: {best_AUC:.4f}.\")\n",
    "    return train_losses, val_losses, prob_list, predict_labels, best_AUC\n",
    "\n",
    "def evaluate(model, val_dataloader, y_val):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's\n",
    "    performance on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled\n",
    "    # during the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "    \n",
    "    outputs_list = []\n",
    "    y_pred_list = []\n",
    "    probs_list = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_input_tbl, b_labels = batch\n",
    "        b_labels = b_labels.view(-1, 1).float()\n",
    "        b_input_ids, b_input_tbl, b_labels = b_input_ids.to(device), b_input_tbl.to(device), b_labels.to(device)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_input_tbl)\n",
    "            logits = logits.to(device)\n",
    "            \n",
    "        y_val_probs = torch.sigmoid(logits)\n",
    "        \n",
    "        outputs_list.append(logits)\n",
    "        probs_list.append(y_val_probs)\n",
    "        \n",
    "        y_pred_tag = torch.round(y_val_probs)\n",
    "        y_pred_list.append(y_pred_tag)\n",
    "            \n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, b_labels).to(device)\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        #preds = torch.argmax(logits, dim=1).flatten()\n",
    "        preds = (logits > 0)\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "    \n",
    "    a = torch.cat(y_pred_list).cpu().detach().numpy()\n",
    "    p = torch.cat(probs_list).cpu().detach().numpy()\n",
    "\n",
    "    #Convert list of lists to list\n",
    "    y_predict_list = np.ndarray.flatten(a).tolist()\n",
    "    y_probs_list = np.ndarray.flatten(p).tolist()\n",
    "    \n",
    "    val_f1 = f1_score(y_val, y_predict_list)\n",
    "    val_AUC = roc_auc_score(y_val, y_probs_list)\n",
    "    \n",
    "\n",
    "    return val_loss, val_accuracy, val_f1, val_AUC, y_probs_list, y_predict_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e394cd84",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "caeb4f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230\n",
      "6900\n"
     ]
    }
   ],
   "source": [
    "print(len(df_static))\n",
    "print(len(df_series))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1328282a-a885-4f7f-84fb-cab8e6d200a2",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "22e20ba3-edd8-4088-844a-61a4b2a40be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Spo2_Mean',\n",
       " 'Spo2_cov',\n",
       " 'Spo2_Max',\n",
       " 'Spo2_Min',\n",
       " 'Fio2_Mean',\n",
       " 'Fio2_Max',\n",
       " 'Fio2_Min',\n",
       " 'Fio2_cov',\n",
       " 'Hyper_percent']"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract column names from time-series dataset\n",
    "feature_columns = df_series.columns.tolist()[1:]\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d50fd41-85f4-4f3b-8fa7-fd7707579c2a",
   "metadata": {},
   "source": [
    "## Normalized the continues varaible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f5debfda-9e86-499e-8b6e-54b74524b2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "feature = [\"BW\", 'GA']\n",
    "autoscaler = MinMaxScaler()\n",
    "df_static[feature] = autoscaler.fit_transform(df_static[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d0a31989-16c0-41f5-81bd-2d875c8ee5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_static.iloc[:,0:3]\n",
    "con_feature_columns = df_demo.columns.tolist()[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "726f1822-4499-4144-a51e-46e800154fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_static = df_static.sort_values('ohsu_mrn').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "b9dadf45-1a0c-4e60-bf67-a063a9490194",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = df_demo.sort_values('ohsu_mrn').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "dd8ddeb1-1dc1-4751-b34c-b1566875690f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-save the dataset\n",
    "X_time_series = df_series.drop(\"ohsu_mrn\", axis=1)\n",
    "X_static = df_demo.drop(\"ohsu_mrn\", axis=1)\n",
    "y  = df_static.drop(\"ohsu_mrn\", axis=1)['binary_ROP']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81b210f-baf4-4fba-8149-efc6c3dfd6bc",
   "metadata": {},
   "source": [
    "## Reshape Time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c04f296d-63b9-4de3-bc96-801959d7f4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(230, 30, 9)\n"
     ]
    }
   ],
   "source": [
    "# Convert the DataFrame to a NumPy array\n",
    "data_array = X_time_series.to_numpy()\n",
    "\n",
    "# Reshape the NumPy array into the desired shape (230, 30, 9)\n",
    "reshaped_array = data_array.reshape(230, 30, 9)\n",
    "\n",
    "# Check the shape of the reshaped array\n",
    "print(reshaped_array.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f811f886-953f-48fb-ad59-bd54e007aae6",
   "metadata": {},
   "source": [
    "## Generate PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "55b9e424-a526-4d31-b7d7-070e4ac0f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "sequence_data_tensor = torch.Tensor(reshaped_array).float()\n",
    "static_features_tensor = torch.Tensor(X_static.to_numpy()).float()\n",
    "binary_labels_tensor = torch.Tensor(y.to_numpy()).long()\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(sequence_data_tensor, static_features_tensor, binary_labels_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919fa10b-50e7-4345-8b81-d343ea1f76d1",
   "metadata": {},
   "source": [
    "## Generate 5-fold Dataloaders of the multimodal dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6363d43a-5cbf-4d52-a5f4-010c72a05eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold [1/5]\n",
      "Fold [2/5]\n",
      "Fold [3/5]\n",
      "Fold [4/5]\n",
      "Fold [5/5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import random\n",
    "\n",
    "random_seed = 101 \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# Create a list to store the data loaders for each fold\n",
    "fold_data_loaders = []\n",
    "# Create a list to store validation labels for each fold\n",
    "fold_val_labels = []\n",
    "\n",
    "\n",
    "num_folds = 5\n",
    "batch_size = 4\n",
    "stratified_kf = StratifiedKFold(n_splits=num_folds)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_kf.split(np.zeros(len(dataset)), binary_labels_tensor)):\n",
    "    print(f'Fold [{fold + 1}/{num_folds}]')\n",
    "    \n",
    "    train_data = torch.utils.data.Subset(dataset, train_indices)\n",
    "    val_data = torch.utils.data.Subset(dataset, val_indices)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "    \n",
    "    # Add the data loaders to the list\n",
    "    fold_data_loaders.append((train_loader, val_loader))\n",
    "    \n",
    "    # Extract the validation labels from your dataset or labels array\n",
    "    val_labels = binary_labels_tensor[val_indices]\n",
    "    fold_val_labels.append(val_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "30809d03-1ae9-428f-ba5a-07a1953c430e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1]),\n",
       " tensor([0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0]),\n",
       " tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1]),\n",
       " tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0]),\n",
       " tensor([0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0])]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check labels for eahc folder\n",
    "fold_val_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1652aa07-cd12-4506-ac62-6f62a9fe5cb2",
   "metadata": {},
   "source": [
    "## Check the dimensions of data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3f03486f-7577-40e7-969b-d441730dd4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X1:  torch.Size([4, 30, 9])\n",
      "Shape of c:  torch.Size([4, 2])\n",
      "Shape of y:  torch.Size([4]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "for X1, c, y in fold_data_loaders[0][0]:\n",
    "    print(\"Shape of X1: \", X1.shape)\n",
    "    print(\"Shape of c: \", c.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f2bd4-120b-4426-ac96-a02150cefc1b",
   "metadata": {},
   "source": [
    "# Multimodal model with different Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "cdd6a308-4592-4e5f-a80c-34ae38e45ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class multiModal(nn.Module):\n",
    "    \"\"\"\n",
    "    Multimodal Neural Network for combining time-series data and static features.\n",
    "\n",
    "    Args:\n",
    "        input_size_ve (int): The input size for the time-series data (number of features at each time point).\n",
    "        hidden_size_ve (int): The hidden size for the LSTM layers.\n",
    "        static_size (int): The size of the static features.\n",
    "        num_classes (int): The number of output classes.\n",
    "        n_LSTMlayers (int): The number of LSTM layers.\n",
    "        n_node_layer1 (int): The number of nodes in the first fully connected layer.\n",
    "        drop_p (float): The dropout probability for regularization.\n",
    "        bidirectional (bool): Whether the LSTM layers are bidirectional.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 input_size_ve, \n",
    "                 hidden_size_ve, \n",
    "                 static_size, \n",
    "                 num_classes, \n",
    "                 n_LSTMlayers, \n",
    "                 n_node_layer1, \n",
    "                 drop_p, \n",
    "                 bidirectional=False):\n",
    "        \n",
    "        super(multiModal, self).__init__()\n",
    "\n",
    "        self.input_size = input_size_ve\n",
    "        self.hidden_size = hidden_size_ve\n",
    "        self.num_layers = n_LSTMlayers\n",
    "        self.n_node1 = n_node_layer1\n",
    "        self.p = drop_p\n",
    "        self.output = num_classes\n",
    "\n",
    "        # LSTM layer for time-series data\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True)\n",
    "\n",
    "        # Static data size\n",
    "        self.static_size = static_size\n",
    "\n",
    "        # Fully connected layer to combine LSTM and static data\n",
    "        self.out = nn.Linear(self.hidden_size + static_size, self.output)\n",
    "\n",
    "    def forward(self, x1, x_static):\n",
    "        \"\"\"\n",
    "        Forward pass of the multimodal neural network.\n",
    "\n",
    "        Args:\n",
    "            x1 (torch.Tensor): Input time-series data with shape (batch_size, sequence_length, input_size_ve).\n",
    "            x_static (torch.Tensor): Static features with shape (batch_size, static_size).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with shape (batch_size, num_classes).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize LSTM hidden and cell states\n",
    "        hidden_state1 = torch.zeros(self.num_layers, x1.size(0), self.hidden_size).to(device)\n",
    "        cell_state1 = torch.zeros(self.num_layers, x1.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        # Forward propagate through the LSTM\n",
    "        lstm_out, (hn1, cn1) = self.lstm1(x1, (hidden_state1, cell_state1))\n",
    "\n",
    "        # Extract the last hidden state from the LSTM output\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "\n",
    "        # Static data remains unchanged\n",
    "        out2 = x_static\n",
    "\n",
    "        # Concatenate the LSTM output and static data\n",
    "        out = torch.cat([lstm_out, out2], dim=1)\n",
    "\n",
    "        # Pass the concatenated data through a fully connected layer\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787e49d-0358-403d-b732-8f4849157eb0",
   "metadata": {},
   "source": [
    "## Set up the hyperparameters (just shows the optimal setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "2098d824-296f-471a-a49c-a0a5da36ebfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "multiModal(\n",
       "  (lstm1): LSTM(9, 16, batch_first=True)\n",
       "  (out): Linear(in_features=18, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_seed = 101 # or any of your favorite number \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# pretrained_embedding, freeze_embedding, n_hidden, n_LSTMlayers, n_outputs\n",
    "\n",
    "input_size_ve = 9\n",
    "hidden_size_ve = 16\n",
    "static_size =2  \n",
    "num_classes =1\n",
    "n_LSTMlayers = 1\n",
    "n_node_layer1 = 16\n",
    "drop_p = 0.5\n",
    "bidirectional=False\n",
    "\n",
    "print(device)\n",
    "\n",
    "model = multiModal(input_size_ve,\n",
    "                   hidden_size_ve,  \n",
    "                   static_size,  \n",
    "                   num_classes, \n",
    "                   n_LSTMlayers,\n",
    "                   n_node_layer1,\n",
    "                   drop_p,\n",
    "                   bidirectional=bidirectional)\n",
    "\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a850a37-2d63-4a71-9dd9-9c7eec8cadd3",
   "metadata": {},
   "source": [
    "## Train and evaluate the model with 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "80aad761-d290-4271-9a56-77db4e0237af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Train Acc |  Val Loss  |  Val Acc  |  Val F1   | Val AUC  | Elapsed \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014836788177490234,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 50,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cb57491f41f43f59a3289177333b942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   0.846804   |   61.96   |  0.831987  |   64.58   | 0.0000   |  0.8519   |   0.14   \n",
      "   2    |   0.844955   |   61.96   |  0.831290  |   64.58   | 0.0000   |  0.8458   |   0.13   \n",
      "   3    |   0.843139   |   61.96   |  0.830603  |   64.58   | 0.0000   |  0.8418   |   0.13   \n",
      "   4    |   0.840882   |   61.96   |  0.829901  |   64.58   | 0.0000   |  0.8337   |   0.13   \n",
      "   5    |   0.838759   |   61.96   |  0.829155  |   62.50   | 0.0000   |  0.8296   |   0.13   \n",
      "   6    |   0.836448   |   63.04   |  0.828404  |   62.50   | 0.0000   |  0.8235   |   0.13   \n",
      "   7    |   0.834074   |   65.76   |  0.827599  |   66.67   | 0.2000   |  0.8235   |   0.13   \n",
      "   8    |   0.831422   |   71.20   |  0.826756  |   66.67   | 0.2000   |  0.8174   |   0.13   \n",
      "   9    |   0.828353   |   75.00   |  0.825805  |   75.00   | 0.5000   |  0.8073   |   0.13   \n",
      "  10    |   0.825163   |   76.63   |  0.824743  |   77.08   | 0.5926   |  0.7992   |   0.13   \n",
      "  11    |   0.821017   |   77.72   |  0.823613  |   79.17   | 0.6429   |  0.7870   |   0.13   \n",
      "  12    |   0.816466   |   79.89   |  0.822179  |   79.17   | 0.6667   |  0.7809   |   0.13   \n",
      "  13    |   0.811014   |   80.43   |  0.820756  |   70.83   | 0.6286   |  0.7789   |   0.13   \n",
      "  14    |   0.803937   |   81.52   |  0.818625  |   68.75   | 0.6111   |  0.7627   |   0.13   \n",
      "  15    |   0.795892   |   80.98   |  0.816049  |   66.67   | 0.6154   |  0.7485   |   0.13   \n",
      "  16    |   0.784831   |   78.80   |  0.813473  |   56.25   | 0.5778   |  0.7302   |   0.13   \n",
      "  17    |   0.771169   |   78.26   |  0.809100  |   60.42   | 0.5909   |  0.7181   |   0.13   \n",
      "  18    |   0.755900   |   76.63   |  0.802625  |   58.33   | 0.5957   |  0.7099   |   0.13   \n",
      "  19    |   0.737550   |   76.63   |  0.797343  |   56.25   | 0.5833   |  0.7160   |   0.13   \n",
      "  20    |   0.720484   |   77.72   |  0.787000  |   60.42   | 0.5909   |  0.7262   |   0.13   \n",
      "  21    |   0.703776   |   77.17   |  0.780714  |   58.33   | 0.5581   |  0.7282   |   0.13   \n",
      "  22    |   0.689329   |   77.72   |  0.764466  |   64.58   | 0.5789   |  0.7282   |   0.13   \n",
      "  23    |   0.675863   |   78.80   |  0.766103  |   58.33   | 0.5581   |  0.7383   |   0.13   \n",
      "  24    |   0.669206   |   77.72   |  0.755161  |   64.58   | 0.5789   |  0.7363   |   0.13   \n",
      "  25    |   0.660730   |   77.72   |  0.755972  |   62.50   | 0.5641   |  0.7302   |   0.13   \n",
      "  26    |   0.649872   |   78.26   |  0.750408  |   64.58   | 0.5789   |  0.7262   |   0.13   \n",
      "  27    |   0.645225   |   77.17   |  0.749650  |   64.58   | 0.5789   |  0.7262   |   0.13   \n",
      "  28    |   0.638804   |   78.26   |  0.750243  |   64.58   | 0.5789   |  0.7201   |   0.13   \n",
      "  29    |   0.633564   |   78.26   |  0.752648  |   62.50   | 0.5641   |  0.7262   |   0.13   \n",
      "  30    |   0.632761   |   78.26   |  0.752677  |   62.50   | 0.5405   |  0.7160   |   0.13   \n",
      "  31    |   0.626476   |   78.80   |  0.754522  |   62.50   | 0.5641   |  0.7181   |   0.13   \n",
      "  32    |   0.622547   |   77.72   |  0.756590  |   62.50   | 0.5641   |  0.7181   |   0.13   \n",
      "  33    |   0.619481   |   78.26   |  0.761897  |   58.33   | 0.5366   |  0.7140   |   0.13   \n",
      "  34    |   0.614248   |   79.35   |  0.761654  |   60.42   | 0.5263   |  0.7140   |   0.13   \n",
      "  35    |   0.617071   |   77.72   |  0.771145  |   58.33   | 0.5366   |  0.7079   |   0.13   \n",
      "  36    |   0.610385   |   79.35   |  0.769161  |   56.25   | 0.5000   |  0.7079   |   0.13   \n",
      "  37    |   0.605569   |   78.80   |  0.772578  |   56.25   | 0.5000   |  0.7059   |   0.13   \n",
      "  38    |   0.602831   |   79.89   |  0.776384  |   58.33   | 0.5128   |  0.6937   |   0.13   \n",
      "  39    |   0.599712   |   80.43   |  0.781772  |   54.17   | 0.4878   |  0.6957   |   0.13   \n",
      "  40    |   0.596372   |   79.35   |  0.785810  |   54.17   | 0.4878   |  0.6978   |   0.13   \n",
      "  41    |   0.595076   |   80.43   |  0.789472  |   54.17   | 0.4878   |  0.6957   |   0.13   \n",
      "  42    |   0.592604   |   79.35   |  0.789766  |   56.25   | 0.5000   |  0.6836   |   0.13   \n",
      "  43    |   0.590682   |   80.43   |  0.794914  |   54.17   | 0.4878   |  0.6897   |   0.13   \n",
      "  44    |   0.587072   |   79.89   |  0.800670  |   56.25   | 0.5000   |  0.6815   |   0.14   \n",
      "  45    |   0.586020   |   79.35   |  0.802037  |   56.25   | 0.5000   |  0.6795   |   0.14   \n",
      "  46    |   0.585225   |   80.43   |  0.801669  |   54.17   | 0.4878   |  0.6836   |   0.13   \n",
      "  47    |   0.582046   |   79.89   |  0.809691  |   54.17   | 0.4878   |  0.6815   |   0.13   \n",
      "  48    |   0.580041   |   79.89   |  0.812681  |   56.25   | 0.5000   |  0.6734   |   0.13   \n",
      "  49    |   0.579263   |   79.89   |  0.820815  |   54.17   | 0.4878   |  0.6815   |   0.13   \n",
      "  50    |   0.576280   |   79.89   |  0.815286  |   54.17   | 0.4878   |  0.6815   |   0.13   \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 79.17%.\n",
      "Training complete! Best AUC: 0.8519.\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Train Acc |  Val Loss  |  Val Acc  |  Val F1   | Val AUC  | Elapsed \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0087738037109375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 50,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7ae8459279646038cdb105ccbfd2ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   0.648190   |   73.37   |  0.538633  |   83.33   | 0.7778   |  0.9148   |   0.13   \n",
      "   2    |   0.642797   |   73.37   |  0.539979  |   83.33   | 0.7778   |  0.9087   |   0.13   \n",
      "   3    |   0.638413   |   75.00   |  0.547418  |   85.42   | 0.8000   |  0.9128   |   0.13   \n",
      "   4    |   0.641257   |   72.83   |  0.550419  |   81.25   | 0.7273   |  0.9108   |   0.13   \n",
      "   5    |   0.637480   |   74.46   |  0.555507  |   81.25   | 0.7273   |  0.9108   |   0.13   \n",
      "   6    |   0.635536   |   73.91   |  0.560650  |   81.25   | 0.7273   |  0.9108   |   0.13   \n",
      "   7    |   0.634178   |   75.54   |  0.558876  |   81.25   | 0.7273   |  0.9108   |   0.13   \n",
      "   8    |   0.636106   |   72.83   |  0.560683  |   81.25   | 0.7273   |  0.9108   |   0.13   \n",
      "   9    |   0.633687   |   73.91   |  0.560739  |   81.25   | 0.7273   |  0.9108   |   0.13   \n",
      "  10    |   0.632526   |   74.46   |  0.561184  |   81.25   | 0.7273   |  0.9047   |   0.13   \n",
      "  11    |   0.632608   |   73.91   |  0.560847  |   81.25   | 0.7273   |  0.9047   |   0.13   \n",
      "  12    |   0.630839   |   74.46   |  0.560128  |   81.25   | 0.7273   |  0.9067   |   0.13   \n",
      "  13    |   0.631802   |   74.46   |  0.562304  |   83.33   | 0.7500   |  0.9067   |   0.14   \n",
      "  14    |   0.632593   |   74.46   |  0.557548  |   79.17   | 0.7059   |  0.9026   |   0.13   \n",
      "  15    |   0.630074   |   74.46   |  0.560883  |   83.33   | 0.7500   |  0.9047   |   0.13   \n",
      "  16    |   0.629901   |   75.00   |  0.565245  |   83.33   | 0.7500   |  0.9067   |   0.13   \n",
      "  17    |   0.630655   |   76.09   |  0.563341  |   83.33   | 0.7500   |  0.9087   |   0.13   \n",
      "  18    |   0.629310   |   75.54   |  0.559299  |   81.25   | 0.7273   |  0.9047   |   0.14   \n",
      "  19    |   0.627952   |   74.46   |  0.560522  |   83.33   | 0.7500   |  0.9047   |   0.13   \n",
      "  20    |   0.627386   |   75.54   |  0.562252  |   83.33   | 0.7500   |  0.9087   |   0.13   \n",
      "  21    |   0.627183   |   75.54   |  0.561268  |   83.33   | 0.7500   |  0.9108   |   0.13   \n",
      "  22    |   0.629003   |   75.00   |  0.560058  |   83.33   | 0.7500   |  0.9087   |   0.13   \n",
      "  23    |   0.629097   |   75.54   |  0.560407  |   83.33   | 0.7500   |  0.9087   |   0.13   \n",
      "  24    |   0.628377   |   75.54   |  0.559095  |   83.33   | 0.7500   |  0.9067   |   0.13   \n",
      "  25    |   0.628120   |   75.00   |  0.560295  |   83.33   | 0.7500   |  0.9087   |   0.13   \n",
      "  26    |   0.626423   |   75.54   |  0.560818  |   81.25   | 0.7273   |  0.9026   |   0.14   \n",
      "  27    |   0.627969   |   75.54   |  0.559144  |   83.33   | 0.7500   |  0.9067   |   0.13   \n",
      "  28    |   0.626138   |   75.54   |  0.558352  |   83.33   | 0.7500   |  0.9047   |   0.15   \n",
      "  29    |   0.624263   |   75.54   |  0.558234  |   81.25   | 0.7273   |  0.9047   |   0.13   \n",
      "  30    |   0.624015   |   75.54   |  0.560681  |   83.33   | 0.7500   |  0.9047   |   0.14   \n",
      "  31    |   0.624402   |   76.09   |  0.558178  |   81.25   | 0.7273   |  0.9047   |   0.13   \n",
      "  32    |   0.625086   |   76.63   |  0.559043  |   83.33   | 0.7500   |  0.9047   |   0.13   \n",
      "  33    |   0.625280   |   75.00   |  0.558604  |   83.33   | 0.7500   |  0.9047   |   0.13   \n",
      "  34    |   0.622873   |   76.63   |  0.558869  |   81.25   | 0.7273   |  0.9026   |   0.13   \n",
      "  35    |   0.623749   |   75.00   |  0.561087  |   83.33   | 0.7500   |  0.9047   |   0.13   \n",
      "  36    |   0.622363   |   75.54   |  0.556384  |   81.25   | 0.7273   |  0.9026   |   0.13   \n",
      "  37    |   0.622599   |   75.54   |  0.558342  |   83.33   | 0.7500   |  0.9026   |   0.13   \n",
      "  38    |   0.623034   |   74.46   |  0.556031  |   81.25   | 0.7273   |  0.9006   |   0.13   \n",
      "  39    |   0.621733   |   76.09   |  0.559560  |   81.25   | 0.7273   |  0.9026   |   0.13   \n",
      "  40    |   0.621690   |   76.09   |  0.557898  |   81.25   | 0.7273   |  0.9026   |   0.13   \n",
      "  41    |   0.623607   |   76.09   |  0.561418  |   83.33   | 0.7500   |  0.9026   |   0.13   \n",
      "  42    |   0.619114   |   76.63   |  0.558933  |   81.25   | 0.7273   |  0.8986   |   0.13   \n",
      "  43    |   0.619998   |   76.09   |  0.560139  |   81.25   | 0.7273   |  0.9006   |   0.13   \n",
      "  44    |   0.621028   |   76.09   |  0.557753  |   81.25   | 0.7273   |  0.8986   |   0.13   \n",
      "  45    |   0.619452   |   76.09   |  0.557796  |   81.25   | 0.7273   |  0.9006   |   0.13   \n",
      "  46    |   0.617969   |   76.09   |  0.558286  |   81.25   | 0.7273   |  0.8986   |   0.14   \n",
      "  47    |   0.618095   |   76.09   |  0.563207  |   83.33   | 0.7333   |  0.9006   |   0.13   \n",
      "  48    |   0.618485   |   76.09   |  0.561870  |   85.42   | 0.7742   |  0.9006   |   0.13   \n",
      "  49    |   0.621323   |   76.09   |  0.563403  |   81.25   | 0.7097   |  0.8986   |   0.13   \n",
      "  50    |   0.623759   |   76.63   |  0.567000  |   81.25   | 0.6897   |  0.9026   |   0.13   \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 85.42%.\n",
      "Training complete! Best AUC: 0.9148.\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Train Acc |  Val Loss  |  Val Acc  |  Val F1   | Val AUC  | Elapsed \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008867025375366211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 50,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860798160d0c4ed4a4923e3ed59635fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   0.625176   |   76.63   |  0.535754  |   79.17   | 0.7059   |  0.9087   |   0.13   \n",
      "   2    |   0.626795   |   76.63   |  0.537569  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "   3    |   0.621863   |   77.17   |  0.542841  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "   4    |   0.623028   |   76.63   |  0.540808  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "   5    |   0.622045   |   77.17   |  0.549181  |   81.25   | 0.7273   |  0.8986   |   0.13   \n",
      "   6    |   0.620299   |   77.17   |  0.542716  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "   7    |   0.620506   |   77.17   |  0.556028  |   81.25   | 0.7273   |  0.8945   |   0.13   \n",
      "   8    |   0.621205   |   77.17   |  0.538390  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "   9    |   0.618764   |   77.17   |  0.551011  |   81.25   | 0.7273   |  0.8945   |   0.13   \n",
      "  10    |   0.619396   |   77.17   |  0.542783  |   81.25   | 0.7429   |  0.8966   |   0.14   \n",
      "  11    |   0.617470   |   77.72   |  0.542187  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "  12    |   0.618711   |   77.72   |  0.540572  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "  13    |   0.618650   |   76.63   |  0.540563  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "  14    |   0.618018   |   77.17   |  0.546810  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  15    |   0.617525   |   76.63   |  0.539342  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "  16    |   0.617477   |   76.63   |  0.546593  |   83.33   | 0.7647   |  0.8945   |   0.13   \n",
      "  17    |   0.616080   |   77.72   |  0.549656  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  18    |   0.616503   |   77.17   |  0.548359  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  19    |   0.616004   |   77.72   |  0.542750  |   81.25   | 0.7429   |  0.8966   |   0.13   \n",
      "  20    |   0.616111   |   76.09   |  0.547403  |   83.33   | 0.7647   |  0.8925   |   0.14   \n",
      "  21    |   0.619243   |   76.63   |  0.559419  |   83.33   | 0.7647   |  0.8945   |   0.13   \n",
      "  22    |   0.616248   |   76.63   |  0.543950  |   81.25   | 0.7429   |  0.8925   |   0.13   \n",
      "  23    |   0.616984   |   77.17   |  0.551890  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  24    |   0.617576   |   75.00   |  0.548389  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  25    |   0.616669   |   77.17   |  0.564427  |   81.25   | 0.7273   |  0.8925   |   0.13   \n",
      "  26    |   0.615387   |   77.17   |  0.541777  |   81.25   | 0.7429   |  0.8945   |   0.13   \n",
      "  27    |   0.615666   |   76.63   |  0.547318  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  28    |   0.612243   |   76.63   |  0.554194  |   83.33   | 0.7647   |  0.8905   |   0.13   \n",
      "  29    |   0.615998   |   77.17   |  0.544408  |   83.33   | 0.7647   |  0.8945   |   0.13   \n",
      "  30    |   0.613760   |   76.63   |  0.543581  |   81.25   | 0.7429   |  0.8945   |   0.13   \n",
      "  31    |   0.616991   |   77.72   |  0.549694  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  32    |   0.614772   |   76.63   |  0.550548  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  33    |   0.612094   |   76.63   |  0.548053  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  34    |   0.614733   |   78.26   |  0.549838  |   83.33   | 0.7647   |  0.8884   |   0.13   \n",
      "  35    |   0.612130   |   77.17   |  0.549639  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  36    |   0.612957   |   77.72   |  0.545013  |   79.17   | 0.7222   |  0.8925   |   0.13   \n",
      "  37    |   0.615165   |   77.72   |  0.547674  |   83.33   | 0.7647   |  0.8925   |   0.13   \n",
      "  38    |   0.613596   |   75.00   |  0.559590  |   81.25   | 0.7273   |  0.8905   |   0.13   \n",
      "  39    |   0.610320   |   76.09   |  0.550976  |   83.33   | 0.7647   |  0.8884   |   0.13   \n",
      "  40    |   0.609498   |   77.72   |  0.556148  |   83.33   | 0.7647   |  0.8884   |   0.13   \n",
      "  41    |   0.610213   |   76.09   |  0.557549  |   83.33   | 0.7647   |  0.8905   |   0.13   \n",
      "  42    |   0.610975   |   77.72   |  0.558402  |   81.25   | 0.7273   |  0.8905   |   0.13   \n",
      "  43    |   0.609062   |   77.72   |  0.549939  |   83.33   | 0.7647   |  0.8884   |   0.14   \n",
      "  44    |   0.610859   |   76.09   |  0.556868  |   81.25   | 0.7273   |  0.8905   |   0.13   \n",
      "  45    |   0.611388   |   78.80   |  0.544264  |   83.33   | 0.7647   |  0.8884   |   0.13   \n",
      "  46    |   0.609493   |   76.09   |  0.553087  |   81.25   | 0.7273   |  0.8905   |   0.13   \n",
      "  47    |   0.608489   |   77.72   |  0.550317  |   83.33   | 0.7647   |  0.8884   |   0.13   \n",
      "  48    |   0.608493   |   75.54   |  0.563511  |   81.25   | 0.7273   |  0.8905   |   0.13   \n",
      "  49    |   0.609967   |   76.63   |  0.558094  |   81.25   | 0.7273   |  0.8905   |   0.13   \n",
      "  50    |   0.606974   |   77.17   |  0.553294  |   83.33   | 0.7647   |  0.8884   |   0.13   \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 83.33%.\n",
      "Training complete! Best AUC: 0.9087.\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Train Acc |  Val Loss  |  Val Acc  |  Val F1   | Val AUC  | Elapsed \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009446859359741211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 50,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911d08dfcac84126a50f8d9a4f591257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   0.578014   |   78.80   |  0.702460  |   72.92   | 0.6667   |  0.7996   |   0.14   \n",
      "   2    |   0.580788   |   76.09   |  0.699864  |   72.92   | 0.6667   |  0.8016   |   0.13   \n",
      "   3    |   0.574856   |   80.43   |  0.701522  |   75.00   | 0.6857   |  0.8016   |   0.13   \n",
      "   4    |   0.572765   |   77.72   |  0.705502  |   75.00   | 0.6857   |  0.8016   |   0.13   \n",
      "   5    |   0.572756   |   78.80   |  0.708878  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "   6    |   0.571633   |   80.43   |  0.709776  |   72.92   | 0.6667   |  0.8016   |   0.13   \n",
      "   7    |   0.573498   |   77.17   |  0.712253  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "   8    |   0.567634   |   80.98   |  0.711381  |   75.00   | 0.6857   |  0.8016   |   0.13   \n",
      "   9    |   0.569168   |   80.98   |  0.713356  |   75.00   | 0.6857   |  0.8016   |   0.13   \n",
      "  10    |   0.568759   |   80.43   |  0.714518  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "  11    |   0.568610   |   79.35   |  0.715851  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "  12    |   0.570215   |   80.98   |  0.715738  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "  13    |   0.567651   |   80.43   |  0.718965  |   72.92   | 0.6667   |  0.7976   |   0.13   \n",
      "  14    |   0.566068   |   78.80   |  0.718630  |   72.92   | 0.6667   |  0.7976   |   0.13   \n",
      "  15    |   0.566529   |   80.43   |  0.718686  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "  16    |   0.567429   |   77.17   |  0.727563  |   72.92   | 0.6667   |  0.7976   |   0.13   \n",
      "  17    |   0.564137   |   78.80   |  0.717757  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "  18    |   0.564369   |   79.89   |  0.722338  |   72.92   | 0.6667   |  0.7937   |   0.13   \n",
      "  19    |   0.564226   |   80.43   |  0.718334  |   72.92   | 0.6667   |  0.7976   |   0.13   \n",
      "  20    |   0.565109   |   79.35   |  0.717881  |   72.92   | 0.6667   |  0.7976   |   0.15   \n",
      "  21    |   0.566095   |   79.35   |  0.720135  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  22    |   0.571383   |   77.72   |  0.728172  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  23    |   0.564111   |   77.72   |  0.722121  |   72.92   | 0.6667   |  0.7937   |   0.13   \n",
      "  24    |   0.567584   |   80.43   |  0.722332  |   72.92   | 0.6667   |  0.7917   |   0.13   \n",
      "  25    |   0.564411   |   78.26   |  0.721673  |   72.92   | 0.6667   |  0.7937   |   0.13   \n",
      "  26    |   0.559208   |   78.26   |  0.720492  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  27    |   0.558951   |   79.89   |  0.720777  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  28    |   0.557821   |   78.80   |  0.721665  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  29    |   0.563849   |   79.35   |  0.725873  |   72.92   | 0.6667   |  0.7917   |   0.14   \n",
      "  30    |   0.559749   |   77.72   |  0.725430  |   72.92   | 0.6667   |  0.7937   |   0.13   \n",
      "  31    |   0.558076   |   78.80   |  0.721733  |   75.00   | 0.6857   |  0.7956   |   0.13   \n",
      "  32    |   0.556511   |   78.80   |  0.724188  |   72.92   | 0.6667   |  0.7937   |   0.13   \n",
      "  33    |   0.555582   |   78.80   |  0.721883  |   72.92   | 0.6667   |  0.7976   |   0.13   \n",
      "  34    |   0.556896   |   78.80   |  0.725472  |   72.92   | 0.6667   |  0.7917   |   0.13   \n",
      "  35    |   0.556674   |   77.72   |  0.723216  |   75.00   | 0.6857   |  0.7956   |   0.13   \n",
      "  36    |   0.555837   |   79.35   |  0.722300  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "  37    |   0.553960   |   79.35   |  0.724033  |   72.92   | 0.6667   |  0.7976   |   0.13   \n",
      "  38    |   0.555054   |   79.35   |  0.725937  |   72.92   | 0.6667   |  0.7976   |   0.13   \n",
      "  39    |   0.554386   |   79.35   |  0.728989  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  40    |   0.556815   |   78.26   |  0.723430  |   75.00   | 0.6857   |  0.7976   |   0.13   \n",
      "  41    |   0.556688   |   80.43   |  0.729418  |   72.92   | 0.6667   |  0.7917   |   0.13   \n",
      "  42    |   0.552268   |   77.17   |  0.725544  |   72.92   | 0.6667   |  0.7976   |   0.13   \n",
      "  43    |   0.553579   |   79.89   |  0.725177  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  44    |   0.554513   |   77.72   |  0.726311  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  45    |   0.550444   |   79.89   |  0.722041  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "  46    |   0.551130   |   79.89   |  0.724110  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "  47    |   0.556038   |   79.35   |  0.726746  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  48    |   0.550465   |   80.43   |  0.725752  |   72.92   | 0.6667   |  0.7956   |   0.13   \n",
      "  49    |   0.551553   |   79.89   |  0.726409  |   72.92   | 0.6667   |  0.7917   |   0.13   \n",
      "  50    |   0.550879   |   79.35   |  0.724186  |   72.92   | 0.6667   |  0.7996   |   0.13   \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 75.00%.\n",
      "Training complete! Best AUC: 0.8016.\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Train Loss  | Train Acc |  Val Loss  |  Val Acc  |  Val F1   | Val AUC  | Elapsed \n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009154796600341797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 50,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cdd7650c05e42188e0d3602eabd5ab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1    |   0.625709   |   75.54   |  0.427143  |   89.58   | 0.8387   |  0.9484   |   0.14   \n",
      "   2    |   0.628147   |   76.63   |  0.424018  |   89.58   | 0.8387   |  0.9464   |   0.14   \n",
      "   3    |   0.622123   |   75.54   |  0.420519  |   89.58   | 0.8387   |  0.9464   |   0.13   \n",
      "   4    |   0.622384   |   76.09   |  0.431552  |   89.58   | 0.8387   |  0.9444   |   0.13   \n",
      "   5    |   0.619587   |   75.54   |  0.420066  |   89.58   | 0.8387   |  0.9464   |   0.13   \n",
      "   6    |   0.619648   |   76.09   |  0.430406  |   89.58   | 0.8387   |  0.9484   |   0.13   \n",
      "   7    |   0.620144   |   75.54   |  0.430068  |   89.58   | 0.8387   |  0.9464   |   0.13   \n",
      "   8    |   0.620715   |   76.63   |  0.434459  |   89.58   | 0.8387   |  0.9444   |   0.13   \n",
      "   9    |   0.621162   |   75.54   |  0.429845  |   85.42   | 0.8125   |  0.9405   |   0.13   \n",
      "  10    |   0.618333   |   76.09   |  0.428780  |   89.58   | 0.8387   |  0.9464   |   0.14   \n",
      "  11    |   0.618253   |   75.54   |  0.434232  |   89.58   | 0.8387   |  0.9444   |   0.14   \n",
      "  12    |   0.618770   |   77.72   |  0.425280  |   89.58   | 0.8387   |  0.9464   |   0.13   \n",
      "  13    |   0.617693   |   77.17   |  0.437509  |   89.58   | 0.8387   |  0.9444   |   0.13   \n",
      "  14    |   0.618400   |   76.09   |  0.439625  |   89.58   | 0.8387   |  0.9425   |   0.13   \n",
      "  15    |   0.620562   |   77.72   |  0.427219  |   85.42   | 0.8125   |  0.9444   |   0.13   \n",
      "  16    |   0.622320   |   77.17   |  0.432749  |   89.58   | 0.8387   |  0.9444   |   0.13   \n",
      "  17    |   0.620530   |   76.09   |  0.442946  |   89.58   | 0.8387   |  0.9444   |   0.14   \n",
      "  18    |   0.618594   |   77.17   |  0.435311  |   89.58   | 0.8387   |  0.9425   |   0.13   \n",
      "  19    |   0.617885   |   76.09   |  0.430812  |   89.58   | 0.8387   |  0.9444   |   0.13   \n",
      "  20    |   0.613837   |   77.17   |  0.436282  |   89.58   | 0.8387   |  0.9425   |   0.13   \n",
      "  21    |   0.615240   |   77.17   |  0.445941  |   89.58   | 0.8387   |  0.9425   |   0.13   \n",
      "  22    |   0.616961   |   76.63   |  0.448096  |   89.58   | 0.8387   |  0.9425   |   0.13   \n",
      "  23    |   0.617279   |   77.72   |  0.435202  |   85.42   | 0.8125   |  0.9425   |   0.14   \n",
      "  24    |   0.614283   |   78.26   |  0.437832  |   85.42   | 0.8125   |  0.9425   |   0.13   \n",
      "  25    |   0.613517   |   77.17   |  0.443241  |   89.58   | 0.8387   |  0.9425   |   0.14   \n",
      "  26    |   0.615014   |   77.17   |  0.459102  |   89.58   | 0.8387   |  0.9385   |   0.13   \n",
      "  27    |   0.616544   |   77.17   |  0.436855  |   85.42   | 0.8125   |  0.9425   |   0.19   \n",
      "  28    |   0.611701   |   78.26   |  0.450266  |   89.58   | 0.8387   |  0.9405   |   0.15   \n",
      "  29    |   0.610367   |   78.26   |  0.450528  |   85.42   | 0.8125   |  0.9405   |   0.13   \n",
      "  30    |   0.611182   |   78.80   |  0.434427  |   83.33   | 0.7879   |  0.9425   |   0.13   \n",
      "  31    |   0.615842   |   77.17   |  0.447363  |   83.33   | 0.7879   |  0.9405   |   0.13   \n",
      "  32    |   0.611353   |   78.80   |  0.445931  |   83.33   | 0.7879   |  0.9405   |   0.14   \n",
      "  33    |   0.609597   |   79.35   |  0.447127  |   83.33   | 0.7879   |  0.9385   |   0.14   \n",
      "  34    |   0.608178   |   78.26   |  0.452067  |   85.42   | 0.8125   |  0.9385   |   0.13   \n",
      "  35    |   0.611010   |   78.26   |  0.445211  |   83.33   | 0.7879   |  0.9385   |   0.13   \n",
      "  36    |   0.610565   |   78.80   |  0.441861  |   83.33   | 0.7879   |  0.9385   |   0.14   \n",
      "  37    |   0.609058   |   78.80   |  0.450645  |   85.42   | 0.8125   |  0.9385   |   0.13   \n",
      "  38    |   0.610009   |   79.35   |  0.448933  |   83.33   | 0.7879   |  0.9385   |   0.13   \n",
      "  39    |   0.608732   |   78.26   |  0.443925  |   83.33   | 0.7879   |  0.9385   |   0.13   \n",
      "  40    |   0.607685   |   79.35   |  0.453811  |   83.33   | 0.7879   |  0.9385   |   0.13   \n",
      "  41    |   0.609299   |   77.72   |  0.439750  |   83.33   | 0.7879   |  0.9385   |   0.14   \n",
      "  42    |   0.606285   |   78.80   |  0.442710  |   83.33   | 0.7879   |  0.9385   |   0.13   \n",
      "  43    |   0.611490   |   78.26   |  0.457757  |   85.42   | 0.8125   |  0.9385   |   0.14   \n",
      "  44    |   0.605599   |   78.80   |  0.446893  |   83.33   | 0.7879   |  0.9345   |   0.13   \n",
      "  45    |   0.606480   |   78.80   |  0.445525  |   83.33   | 0.7879   |  0.9365   |   0.13   \n",
      "  46    |   0.606583   |   79.35   |  0.442706  |   83.33   | 0.7879   |  0.9385   |   0.13   \n",
      "  47    |   0.606435   |   77.72   |  0.444573  |   83.33   | 0.7879   |  0.9365   |   0.14   \n",
      "  48    |   0.606410   |   77.17   |  0.454105  |   83.33   | 0.7879   |  0.9345   |   0.14   \n",
      "  49    |   0.604851   |   78.26   |  0.451919  |   83.33   | 0.7879   |  0.9345   |   0.13   \n",
      "  50    |   0.605543   |   76.63   |  0.464777  |   85.42   | 0.8125   |  0.9385   |   0.13   \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 89.58%.\n",
      "Training complete! Best AUC: 0.9484.\n"
     ]
    }
   ],
   "source": [
    "random_seed = 101 # or any of your favorite number \n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)\n",
    "\n",
    "\"\"\"Parameters\n",
    "(model, optimizer, loss_fn, train_dataloader, val_dataloader=None, epochs=10)\n",
    "1. model\n",
    "2. optimizer\n",
    "3. loss_fn\n",
    "4. train_dataloader\n",
    "5. val_dataloader\n",
    "6. epochs \n",
    "\"\"\"\n",
    "# Specify loss function\n",
    "\n",
    "#BATCH_SIZE = 16\n",
    "epochs = 50\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "#1e-6 1e-3\n",
    "p_weight = torch.tensor([1.64]).to(device)\n",
    "\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight = p_weight)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay = WEIGHT_DECAY)\n",
    "\n",
    "num_folds = 5\n",
    "\n",
    "sum_AUROC = []\n",
    "\n",
    "for i in range(num_folds):\n",
    "    \n",
    "    y_val = fold_val_labels[i]\n",
    "    train_dataloader = fold_data_loaders[i][0]\n",
    "    test_dataloader = fold_data_loaders[i][1]\n",
    "    \n",
    "    train_losses, val_loss, best_p, best_label, best_AUC = train(model, optimizer, loss_fn, train_dataloader, test_dataloader, y_val, epochs)\n",
    "    \n",
    "    sum_AUROC.append(best_AUC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "dae7ee7e-6954-4efb-89df-b6dc7d2cf6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8850912778904665"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sum_AUROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a794dc-b909-4ec0-8a8c-6b0c66081e6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
